model:
    n_embd: 64
    n_layer: 3 # 3 layers of transformers (GPT2) 
    n_head: 2
